{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Проект-1\" data-toc-modified-id=\"Проект-1-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Проект 1</a></div><div class=\"lev1 toc-item\"><a href=\"#Спрогнозировать-пол-и-возрастную-категорию-интернет-пользователей-по-логу-посещения-сайтов\" data-toc-modified-id=\"Спрогнозировать-пол-и-возрастную-категорию-интернет-пользователей-по-логу-посещения-сайтов-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Спрогнозировать пол и возрастную категорию интернет-пользователей по логу посещения сайтов</a></div><div class=\"lev3 toc-item\"><a href=\"#Задача\" data-toc-modified-id=\"Задача-201\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Задача</a></div><div class=\"lev3 toc-item\"><a href=\"#Обработка-данных-на-вход\" data-toc-modified-id=\"Обработка-данных-на-вход-202\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>Обработка данных на вход</a></div><div class=\"lev3 toc-item\"><a href=\"#Очистка-данных-и-feature-engineering\" data-toc-modified-id=\"Очистка-данных-и-feature-engineering-203\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>Очистка данных и feature engineering</a></div><div class=\"lev3 toc-item\"><a href=\"#Деление-на-train-и-test-сеты,-обучение-модели,-предсказания-для-test-сета\" data-toc-modified-id=\"Деление-на-train-и-test-сеты,-обучение-модели,-предсказания-для-test-сета-204\"><span class=\"toc-item-num\">2.0.4&nbsp;&nbsp;</span>Деление на train и test сеты, обучение модели, предсказания для test-сета</a></div><div class=\"lev3 toc-item\"><a href=\"#Обработка-данных-на-выход\" data-toc-modified-id=\"Обработка-данных-на-выход-205\"><span class=\"toc-item-num\">2.0.5&nbsp;&nbsp;</span>Обработка данных на выход</a></div><div class=\"lev4 toc-item\"><a href=\"#Пример-выходного-файла:\" data-toc-modified-id=\"Пример-выходного-файла:-2051\"><span class=\"toc-item-num\">2.0.5.1&nbsp;&nbsp;</span>Пример выходного файла:</a></div><div class=\"lev3 toc-item\"><a href=\"#Подсказки\" data-toc-modified-id=\"Подсказки-206\"><span class=\"toc-item-num\">2.0.6&nbsp;&nbsp;</span>Подсказки</a></div><div class=\"lev3 toc-item\"><a href=\"#Проверка\" data-toc-modified-id=\"Проверка-207\"><span class=\"toc-item-num\">2.0.7&nbsp;&nbsp;</span>Проверка</a></div><div class=\"lev2 toc-item\"><a href=\"#Ваше-решение-здесь\" data-toc-modified-id=\"Ваше-решение-здесь-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Ваше решение здесь</a></div><div class=\"lev2 toc-item\"><a href='#Добавим-класс-\"пол-возраст\"' data-toc-modified-id='Добавим-класс-\"пол-возраст\"-22'><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Добавим класс \"пол-возраст\"</a></div><div class=\"lev2 toc-item\"><a href=\"#Парсим-url\" data-toc-modified-id=\"Парсим-url-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Парсим url</a></div><div class=\"lev2 toc-item\"><a href=\"#Разобьем-на-обучение-и-тест\" data-toc-modified-id=\"Разобьем-на-обучение-и-тест-24\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Разобьем на обучение и тест</a></div><div class=\"lev2 toc-item\"><a href=\"#Классифицируем\" data-toc-modified-id=\"Классифицируем-25\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Классифицируем</a></div><div class=\"lev1 toc-item\"><a href=\"#Содержимое-сайтов\" data-toc-modified-id=\"Содержимое-сайтов-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Содержимое сайтов</a></div><div class=\"lev1 toc-item\"><a href=\"#Категории-сайтов\" data-toc-modified-id=\"Категории-сайтов-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Категории сайтов</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"50px\" align=\"left\" style=\"margin-right:20px\" src=\"http://data.newprolab.com/public-newprolab-com/npl_logo.png\"> <b>New Professions Lab</b> <br /> Специалист по большим данным"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Спрогнозировать пол и возрастную категорию интернет-пользователей по логу посещения сайтов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"110px\" align=\"left\" src=\"http://data.newprolab.com/public-newprolab-com/project01_img0.png?img\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/newprolab/content_bigdata6\"><img align=\"left\" src=\"http://data.newprolab.com/public-newprolab-com/npl.svg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из задач DMP-системы состоит в том, чтобы по разрозненным даннным, таким, как посещения неким пользователем сайтов, классифицировать его и присвоить ему определённую категорию: пол, возраст, интересы и так далее. В дальнейшем составляется портрет, или профиль, пользователя, на основе которого ему более таргетированно показывается реклама в интернете."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя доступный набор данных о посещении страниц у одной части пользователей, сделать прогноз относительно **пола и возрастной категории** другой части пользователей. Угадывание (hit) - правильное предсказание и пола, и возрастной категории одновременно.\n",
    "\n",
    "Мы не ограничиваем вас в выборе инструментов и методов работы с данными. Используйте любые эвристики, внешние источники, парсинг контента страниц — всё, что поможет вам выполнить задачу. Единственное ограничение — никаких ручных действий. Руками проставлять классы нельзя.\n",
    "\n",
    "Поскольку это ваш проект, который мы наверняка захотите показать другим, уделите его оформлению достаточно времени. Мы рекомендуем сделать весь проект в этом ноутбуке. Снизу, под заданием, вы сможете описать ваше решение.\n",
    "\n",
    "⏰ **Дедлайн: 02 ноября 2017, 23:59**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlretrieve, unquote\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка данных на вход"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выполнения работы вам следует взять файл http://data.newprolab.com/data-newprolab-com/project01/gender_age_dataset.txt и положить к себе в директорию `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# First, we download the input data for the project:\n",
    "\n",
    "data_dir = 'data'\n",
    "filename = 'gender_age_dataset.txt'\n",
    "file_path = '/'.join([data_dir,filename])\n",
    "url = 'http://data.newprolab.com/data-newprolab-com/project01/' + filename\n",
    "\n",
    "if not os.path.isdir(data_dir): os.mkdir(data_dir)\n",
    "if not os.path.isfile(file_path):\n",
    "    print('Downloading ' + filename + '...')\n",
    "    urlretrieve(url, file_path)\n",
    "    print('Download completed')\n",
    "\n",
    "# Wait until you see that all files have been downloaded.\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Он содержит данные о посещении сайтов ~40 000 пользователей, при этом по некоторым из них (~ 35 000) известны их пол и возрастная категория, а по 5 000 - эта информация не известна. В файле есть 4 поля:\n",
    "* **gender** - пол, принимающий значения `M` (male - мужчина), `F` (female - женщина), `-` (пол неизвестен);\n",
    "* **age** - возраст, представленный в виде диапазона x-y (строковый тип), или `-` (возрастная категория неизвестна);\n",
    "* **uid** - идентификатор пользователя, строковая переменная;\n",
    "* **user_json** - поле json, в котором содержатся записи о посещении сайтов этим пользователем `(url, timestamp)`.\n",
    "\n",
    "Первое, что обычно делают в таких случаях, — исследуют имеющийся датасет и разбираются, какие же данные мы получили."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим весь датасет в pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И теперь попробуем понять, что у нас есть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>uid</th>\n",
       "      <th>user_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>18-24</td>\n",
       "      <td>d50192e5-c44e-4ae8-ae7a-7cfe67c8b777</td>\n",
       "      <td>{\"visits\": [{\"url\": \"http://zebra-zoya.ru/2000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>25-34</td>\n",
       "      <td>d502331d-621e-4721-ada2-5d30b2c3801f</td>\n",
       "      <td>{\"visits\": [{\"url\": \"http://sweetrading.ru/?p=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>25-34</td>\n",
       "      <td>d50237ea-747e-48a2-ba46-d08e71dddfdb</td>\n",
       "      <td>{\"visits\": [{\"url\": \"http://ru.oriflame.com/pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>25-34</td>\n",
       "      <td>d502f29f-d57a-46bf-8703-1cb5f8dcdf03</td>\n",
       "      <td>{\"visits\": [{\"url\": \"http://translate-tattoo.r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>&gt;=55</td>\n",
       "      <td>d503c3b2-a0c2-4f47-bb27-065058c73008</td>\n",
       "      <td>{\"visits\": [{\"url\": \"https://mail.rambler.ru/#...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender    age                                   uid  \\\n",
       "0      F  18-24  d50192e5-c44e-4ae8-ae7a-7cfe67c8b777   \n",
       "1      M  25-34  d502331d-621e-4721-ada2-5d30b2c3801f   \n",
       "2      F  25-34  d50237ea-747e-48a2-ba46-d08e71dddfdb   \n",
       "3      F  25-34  d502f29f-d57a-46bf-8703-1cb5f8dcdf03   \n",
       "4      M   >=55  d503c3b2-a0c2-4f47-bb27-065058c73008   \n",
       "\n",
       "                                           user_json  \n",
       "0  {\"visits\": [{\"url\": \"http://zebra-zoya.ru/2000...  \n",
       "1  {\"visits\": [{\"url\": \"http://sweetrading.ru/?p=...  \n",
       "2  {\"visits\": [{\"url\": \"http://ru.oriflame.com/pr...  \n",
       "3  {\"visits\": [{\"url\": \"http://translate-tattoo.r...  \n",
       "4  {\"visits\": [{\"url\": \"https://mail.rambler.ru/#...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что содержится в `user_json`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"visits\": [{\"url\": \"http://zebra-zoya.ru/200028-chehol-organayzer-dlja-macbook-11-grid-it.html?utm_campaign=397720794&utm_content=397729344&utm_medium=cpc&utm_source=begun\", \"timestamp\": 1419688144068}, {\"url\": \"http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story\", \"timestamp\": 1426666298001}, {\"url\": \"http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html\", \"timestamp\": 1426666298000}, {\"url\": \"http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story\", \"timestamp\": 1426661722001}, {\"url\": \"http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html\", \"timestamp\": 1426661722000}]}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0].user_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что это некая сериализованная json-строка, которую можно легко разобрать через модуль `json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'visits': [{'timestamp': 1419688144068,\n",
       "   'url': 'http://zebra-zoya.ru/200028-chehol-organayzer-dlja-macbook-11-grid-it.html?utm_campaign=397720794&utm_content=397729344&utm_medium=cpc&utm_source=begun'},\n",
       "  {'timestamp': 1426666298001,\n",
       "   'url': 'http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story'},\n",
       "  {'timestamp': 1426666298000,\n",
       "   'url': 'http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html'},\n",
       "  {'timestamp': 1426661722001,\n",
       "   'url': 'http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story'},\n",
       "  {'timestamp': 1426661722000,\n",
       "   'url': 'http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html'}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(df.iloc[0].user_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методом `pandas.DataFrame.apply` (хотя не только им) можно применить операцию десериализации json-строк ко всему датасету. Рекомендуем почитать [документацию по методу apply](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html).\n",
    "\n",
    "Работая с подобными операциями, обратите внимание на kwargs-аргумент `axis`. Часто, забыв его указать, вы примените операцию не к ряду (строке), а к столбцу, что вряд ли входит в ваши планы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['user_json'] = df.apply(lambda r: json.loads(r['user_json']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Очистка данных и feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очистка данных и генерация новых фич составит значительную часть вашей работы. Именно здесь вы и должны продемонстрировать знания и креативность: чем лучше окажутся ваши фичи и чем лучше сможете убрать шум из датасета, тем лучших результатов вы достигнете."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из первых вещей, которые можно попробовать — это вытащить домены и использовать их в качестве признаков. Можно воспользоваться функцией:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def url2domain(url):\n",
    "    url = re.sub('(http(s)*://)+', 'http://', url)\n",
    "    parsed_url = urlparse(unquote(url.strip()))\n",
    "    if parsed_url.scheme not in ['http','https']: return None\n",
    "    netloc = re.search(\"(?:www\\.)?(.*)\", parsed_url.netloc).group(1)\n",
    "    if netloc is not None: return str(netloc.encode('utf8')).strip()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку эта часть и есть ваша работа, мы не станем раскрывать все секреты (хотя несколько советов мы всё же дали, посмотрите ниже в разделе Подсказки)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Деление на train и test сеты, обучение модели, предсказания для test-сета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте теперь оценим размер нашего train и test сетов. Train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36138"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[~((df.gender == '-') & (df.age == '-'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[(df.gender == '-') & (df.age == '-')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41138"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df) # Весь датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда вы очистили данные и сгенерировали признаки, которые можно дать на вход алгоритму, следующий этап — это разделить данные на тренировочную и тестовую выборки. Сохраните train и test выборки в отдельных файлах, используя метод `pandas.DataFrame.to_csv`. Либо просто сделайте два датафрейма: `train_df` и `test_df`. Обучите модель на ваш выбор, оцените результат, подумайте, как можно его улучшить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка данных на выход\n",
    "Выходной файл должен быть расположен в корне вашей директории в файле `project01_gender-age.csv`. Чекер будет брать файл именно оттуда.\n",
    "\n",
    "Файл должен содержать три поля: `uid` (строковый формат), `gender` (строковый формат) и `age` (строковый формат). \n",
    "\n",
    "В файле должны быть только те пользователи, у которых пол и возрастная категория изначально неизвестны, и они должны быть **отсортированы по UID по возрастанию значений лексикографически.**\n",
    "\n",
    "**Важное замечание!** Вы должны дать прогноз хотя бы по 50% пользователей, у которых изначально не указан пол и возрастная категория. Иными словами, вы можете оставить неопределенными не более 50% изначально неопределенных пользователей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример выходного файла:\n",
    "\n",
    "```uid\tgender\tage\n",
    "123\tF\t18-24\n",
    "456\tM\t25-34\n",
    "789\t-\t-\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файл обязательно должен содержать шапку, указанную выше, и все 5 000 записей по неизвестным пользователям. Итого: 5 001 строка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подсказки\n",
    "\n",
    "1. Есть много различных способов решить данную задачу: можно просто хорошо поработать с урлами и доменами, можно пропарсить содержимое этих урлов (заголовки, текст и т.д.) и воспользоваться неким векторизатором типа TF\\*IDF для генерации дополнительных фич, которые уже в дальнейшем вы подадите на вход ML-алгоритму, можно сделать тематическое моделирование (LDA, BigARTM) сайтов и использовать одну или несколько тем в качестве фич.\n",
    "\n",
    "2. Возможно, что данные грязные и их нужно дополнительно обработать. Спецсимволы, кириллические домены? Уделите этому этапу достаточно времени: здесь чистота датасета важнее, чем выбор алгоритма.\n",
    "\n",
    "3. Часто бывает, что лучшее решение с точки зрения результата — оно же самое простое. Попробуйте сначала простые способы, простые алгоритмы, прежде чем переходить к тяжёлой артиллерии. Один из вариантов — начать с небольшого RandomForest.\n",
    "\n",
    "4. Вам почти наверняка понадобится что-то из пакета sklearn. [Документация](http://scikit-learn.org/stable/user_guide.html) — ваш лучший друг.\n",
    "\n",
    "5. Вы можете сначала предсказать пол, а затем возраст, либо сразу и то, и другое. Экспериментируйте.\n",
    "\n",
    "6. В Python 2.7 возможны проблемы с юникодом. Способы решения существуют — обращайтесь в slack за советами.\n",
    "\n",
    "7. Объединяйтесь в команды. Так гораздо веселее и интереснее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка\n",
    "Проверка осуществляется из [Личного кабинета](http://lk.newprolab.com/lab/project01). По файлу будет определяться доля правильно спрогнозированных пользователей (у которых правильно указаны и пол, и возрастная категория).\n",
    "\n",
    "* В поле `part of users with predicted gender + age` - указана доля пользователей, которая была предсказана от общего числа неизвестных пользователей (пример: по 3 000 был сделан прогноз, а всего было неизвестно 5 000, чекер выдаст 0.6).\n",
    "\n",
    "* В поле `correctly predicted users / total number of users` - указана доля пользователей, которая была правильно предсказана (совпадает и пол, и возраст) от общего числа всех пользователей (пример: по 3 000 был сделан прогноз, правильно было спрогнозировано 1 500, а всего было неизвестно 5 000, чекер выдаст 0.3)\n",
    "\n",
    "* В поле `correctly predicted users / number of predicted users` - указана доля пользователей, которая была правильно предсказана (совпадает и пол, и возраст) от общего числа предсказанных пользователей (пример: по 3 000 был сделан прогноз, из них правильно предсказано 1 500, чекер выдаст 0.5).\n",
    "\n",
    "**Если доля в последнем поле превысит порог 0.28, то проект будет засчитан.**\n",
    "\n",
    "Лучшей команде, набравшей максимальный результат, мы подарим специальный приз, о котором скажем позднее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ваше решение здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что еще можно попробовать:\n",
    "\n",
    "Отдельно выделить поисковики и запросы на них\n",
    "\n",
    "Частые подстроки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Добавим класс \"пол-возраст\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>uid</th>\n",
       "      <th>user_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>18-24</td>\n",
       "      <td>d50192e5-c44e-4ae8-ae7a-7cfe67c8b777</td>\n",
       "      <td>{'visits': [{'url': 'http://zebra-zoya.ru/2000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>25-34</td>\n",
       "      <td>d502331d-621e-4721-ada2-5d30b2c3801f</td>\n",
       "      <td>{'visits': [{'url': 'http://sweetrading.ru/?p=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>25-34</td>\n",
       "      <td>d50237ea-747e-48a2-ba46-d08e71dddfdb</td>\n",
       "      <td>{'visits': [{'url': 'http://ru.oriflame.com/pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>25-34</td>\n",
       "      <td>d502f29f-d57a-46bf-8703-1cb5f8dcdf03</td>\n",
       "      <td>{'visits': [{'url': 'http://translate-tattoo.r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>&gt;=55</td>\n",
       "      <td>d503c3b2-a0c2-4f47-bb27-065058c73008</td>\n",
       "      <td>{'visits': [{'url': 'https://mail.rambler.ru/#...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender    age                                   uid  \\\n",
       "0      F  18-24  d50192e5-c44e-4ae8-ae7a-7cfe67c8b777   \n",
       "1      M  25-34  d502331d-621e-4721-ada2-5d30b2c3801f   \n",
       "2      F  25-34  d50237ea-747e-48a2-ba46-d08e71dddfdb   \n",
       "3      F  25-34  d502f29f-d57a-46bf-8703-1cb5f8dcdf03   \n",
       "4      M   >=55  d503c3b2-a0c2-4f47-bb27-065058c73008   \n",
       "\n",
       "                                           user_json  \n",
       "0  {'visits': [{'url': 'http://zebra-zoya.ru/2000...  \n",
       "1  {'visits': [{'url': 'http://sweetrading.ru/?p=...  \n",
       "2  {'visits': [{'url': 'http://ru.oriflame.com/pr...  \n",
       "3  {'visits': [{'url': 'http://translate-tattoo.r...  \n",
       "4  {'visits': [{'url': 'https://mail.rambler.ru/#...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сведем задачу к многоклассовой классификации. Введем новый класс - \"пол-возраст\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df.gender + df.age)\n",
    "\n",
    "df['class'] = le.transform(df.gender + df.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['--', 'F18-24', 'F25-34', 'F35-44', 'F45-54', 'F>=55', 'M18-24',\n",
       "       'M25-34', 'M35-44', 'M45-54', 'M>=55'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Парсим url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlsplit\n",
    "\n",
    "def url2domain2(url):\n",
    "    url = url.lower()\n",
    "    url = re.sub('(http(s)*://)+', 'http://', url)\n",
    "    parsed_url = urlparse(unquote(url.strip()))\n",
    "    if parsed_url.scheme not in ['http','https']: \n",
    "        return None\n",
    "    netloc = re.search(\"(?:www\\.)?(.*)\", parsed_url.netloc).group(1)\n",
    "    if netloc is not None: \n",
    "        netloc = netloc.strip()\n",
    "        \n",
    "        # Обрабатываем кириллические домены\n",
    "        if '.xn--' in netloc or netloc.startswith('xn--'):\n",
    "            netloc = netloc.encode('idna').decode('idna')\n",
    "            \n",
    "        # Обрабатываем referrer\n",
    "        r = re.match(r'&referrer=(.+)', netloc)\n",
    "        if r:\n",
    "            netloc = r[1]\n",
    "            \n",
    "        return netloc.strip()\n",
    "    return None\n",
    "\n",
    "def get_domain_words(netloc):\n",
    "    if netloc is not None: \n",
    "        netloc = netloc.strip()\n",
    "        \n",
    "        # Обрабатываем кириллические домены\n",
    "        if '.xn--' in netloc or netloc.startswith('xn--'):\n",
    "            netloc = netloc.encode('idna').decode('idna')\n",
    "            \n",
    "        # Обрабатываем referrer\n",
    "        r = re.match(r'&referrer=(.+)', netloc)\n",
    "        if r:\n",
    "            netloc = r[1]\n",
    "            \n",
    "        return re.split(r'\\W|_|-', netloc.strip())\n",
    "    return []\n",
    "    \n",
    "\n",
    "def get_url_words(url):\n",
    "    url = url.lower()\n",
    "    url = re.sub('(http(s)*://)+', 'http://', url)\n",
    "    \n",
    "    if 'yandex.ru/clck/jsredir' in url:\n",
    "        return ''\n",
    "    parsed_url = urlparse(unquote(url.strip()))\n",
    "    if parsed_url.scheme not in ['http','https']: \n",
    "        return None\n",
    "    netloc = re.search(\"(?:www\\.)?(.*)\", parsed_url.netloc).group(1)\n",
    "    \n",
    "    # Разбиваем на слова домены\n",
    "    words = get_domain_words(netloc)\n",
    "    \n",
    "    # Разбиваем на слова параметры запроса\n",
    "    query_words = sum([q.split('=') for q in parsed_url.query.split('&')], [])\n",
    "    query_words = sum([re.split('\\W|_|-', q) for q in query_words], [])\n",
    "    \n",
    "    # Разбиваем путь на слова\n",
    "    path_words = re.split(r'\\W|_|-', parsed_url.path)\n",
    "    \n",
    "    all_words = [w for w in (words + query_words + path_words) if len(w) >= 3 and not re.match(r'^[0-9]+$', w)]\n",
    "    \n",
    "    all_words = list(set(all_words))\n",
    "    \n",
    "    return ' '.join(all_words)\n",
    "\n",
    "def extract_domains2(visits):\n",
    "    \"\"\"\n",
    "    Получить множество всех доменов из лога пользователя\n",
    "    \"\"\"\n",
    "    return {url2domain2(visit['url']) for visit in visits}\n",
    "\n",
    "\n",
    "def extract_words(visits):\n",
    "    \"\"\"\n",
    "    Получить множество всех слов из лога пользователя\n",
    "    \"\"\"\n",
    "    return {get_url_words(visit['url']) for visit in visits}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим примеры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'driver onda dual air v919 boot html com 64gb tablet spain villa fantastic with situated txttitle really sqm detail bedrooms mountain and nice r2186276 has altea txtlistingid property views com asp mayevski com komedij dlya bodrosti adme tvorchestvo radosti kino obychnyh znayut bikini zhenschin chto oni adme kotorye prekrasny svoboda psihologiya diletant news page spain property default com asp wcb php country listar esp tipogestion jhxoc3e tipoinmueble ypeuxhsi2s5vvq cjwkeajwucmobrdmysgsgbdr5j0sjaaxl9abq6qkarhpp58ue0or8qv6hh com gclid catalanholidays lang i1pdbvcgdv8 video youtube com driver cid 109100493117d8f7fe6d2308864e file predstavlyat rossii cherez budet rusjev yadernoe let ssha oruzhie ugrozyi net etc literature mayevski com html news novayagazeta mayevski life blogs com plackartnom vagone kak org moimir spyat zhest facebook com adme php mayevski contact com blppux3hjn lifenews vyizhit kak zhenschinoy the provider user your mirtesen authentication facebook error denied request blog tvorcheskoy failed smartboobs diletant articles literature mayevski com mayevski blogs com vksrc501305 adme tvorchestvo vksrc kulturnaya stolica svoboda narodnoe listings spain property lngstateid com asp z0m8rnryzc pravil genialnogo fotografy adme tvorchestvo snimka yajcami ibigdan stalnymi com lyudi html atfhalfpage adme adfox vyizhit kak zhenschinoy mirtesen blog tvorcheskoy smartboobs article org content html svoboda url custom 2f1053220603 cgi ref click comppath images params rnd 2f20150320 nid bid ntype c47axkgyi0xrbf0ebgauqmax23oft0dup bin 2fspravka height masterh4 2fria xpid rleurl sid index adriver width html http sliceid shipsnowyo com r416062 kitchen txttitle spain property area bungalow and asp refurbished condi detail levels perfect com residential floors the txtlistingid two homo samtsov sapiense literature nevole etc soderzhanye ros com mayevski pci driver dev 14e4 search 16b3 ven dlya nauchnyh detskoj adme zhizn prostyh opytov vecherinki nauka bolshe nalogov platit video chem amerikanets vedomosti economics rossiyanin mayevski photo com aillar msk echo blog intforrent lnguserid page spain dblprice1 lngstateid intforsale intrentduration inttotalbedrooms inttotalbathrooms lngregionid txtsubtype inttotalsleeps txtlistingid listings txtkeyroreferencenumber property dblprice2 intlistingtype lngcountryid com asp spain property com samyj vdohnovenie ulybatelnyj adme mire post pci driver asusfans dev 14e4 html 16b3 ven strannovatym chto chelovechestvo dokazyvayut fotografy adme tvorchestvo bylo kotorye snimkov vsegda google linuxmint com'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(extract_words(df.iloc[20001].user_json['visits']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' qip aquagroup uchastok rubrikator zemelnyy компакт csg иксперия yandsearch сони отзывы yandex text citymap rb16 поселоктургояк bytype russianfood recipes fid com html vostoke dom pages recept allrecipes aspx kuplyu dogovor kupli prost dogovora zemelnogo html obrazec formate prodazhu biznes prodazhi uchastka skachat doc kuplyu dogovor prost zemelnogo html prodazhu biznes uchastka php rid russianfood recipe recipes com text csg нужны земельного участка документы suggest покупки собственника yandsearch reqid yandex для какие yclid other content phone shop source иксперия zglyzwn0lnlhbmrlec5ydtsxmtyynzkynds3mjywotu1nzc7ewfuzgv4lnj1onbyzw1pdw0 svyaznoy сони отзывы comments campaign cpc region medium yandex position type компакт catalog utm smartfoni openstat premium otzivi'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(extract_words(df.iloc[38560].user_json['visits']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим колонку со словами из url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['url_words'] = df.user_json.apply(lambda j: ' '.join(extract_words(j['visits'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>uid</th>\n",
       "      <th>user_json</th>\n",
       "      <th>class</th>\n",
       "      <th>url_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>18-24</td>\n",
       "      <td>d50192e5-c44e-4ae8-ae7a-7cfe67c8b777</td>\n",
       "      <td>{'visits': [{'url': 'http://zebra-zoya.ru/2000...</td>\n",
       "      <td>1</td>\n",
       "      <td>story one chezasite cl4url htc news delay yand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>25-34</td>\n",
       "      <td>d502331d-621e-4721-ada2-5d30b2c3801f</td>\n",
       "      <td>{'visits': [{'url': 'http://sweetrading.ru/?p=...</td>\n",
       "      <td>7</td>\n",
       "      <td>root получить права как android w832 help q2a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>25-34</td>\n",
       "      <td>d50237ea-747e-48a2-ba46-d08e71dddfdb</td>\n",
       "      <td>{'visits': [{'url': 'http://ru.oriflame.com/pr...</td>\n",
       "      <td>2</td>\n",
       "      <td>results biathlon id63324 rusbiathlon results b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>25-34</td>\n",
       "      <td>d502f29f-d57a-46bf-8703-1cb5f8dcdf03</td>\n",
       "      <td>{'visits': [{'url': 'http://translate-tattoo.r...</td>\n",
       "      <td>2</td>\n",
       "      <td>khozyaev ubili spryatali raschlenennye proissh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>&gt;=55</td>\n",
       "      <td>d503c3b2-a0c2-4f47-bb27-065058c73008</td>\n",
       "      <td>{'visits': [{'url': 'https://mail.rambler.ru/#...</td>\n",
       "      <td>10</td>\n",
       "      <td>fosfpsmtdkfqsmta fnsm2e fdsntu2fesmzaw sg4e8xg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender    age                                   uid  \\\n",
       "0      F  18-24  d50192e5-c44e-4ae8-ae7a-7cfe67c8b777   \n",
       "1      M  25-34  d502331d-621e-4721-ada2-5d30b2c3801f   \n",
       "2      F  25-34  d50237ea-747e-48a2-ba46-d08e71dddfdb   \n",
       "3      F  25-34  d502f29f-d57a-46bf-8703-1cb5f8dcdf03   \n",
       "4      M   >=55  d503c3b2-a0c2-4f47-bb27-065058c73008   \n",
       "\n",
       "                                           user_json  class  \\\n",
       "0  {'visits': [{'url': 'http://zebra-zoya.ru/2000...      1   \n",
       "1  {'visits': [{'url': 'http://sweetrading.ru/?p=...      7   \n",
       "2  {'visits': [{'url': 'http://ru.oriflame.com/pr...      2   \n",
       "3  {'visits': [{'url': 'http://translate-tattoo.r...      2   \n",
       "4  {'visits': [{'url': 'https://mail.rambler.ru/#...     10   \n",
       "\n",
       "                                           url_words  \n",
       "0  story one chezasite cl4url htc news delay yand...  \n",
       "1  root получить права как android w832 help q2a ...  \n",
       "2  results biathlon id63324 rusbiathlon results b...  \n",
       "3  khozyaev ubili spryatali raschlenennye proissh...  \n",
       "4  fosfpsmtdkfqsmta fnsm2e fdsntu2fesmzaw sg4e8xg...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разобьем на обучение и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = df[~((df.gender == '-') & (df.age == '-'))]\n",
    "test_df = df[(df.gender == '-') & (df.age == '-')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классифицируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=4, max_df=0.75)\n",
    "tfidf.fit(df.url_words)\n",
    "domain_words_features = tfidf.transform(train_df.url_words)\n",
    "y = train_df['class'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(domain_words_features, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30593986349382035"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls = LogisticRegression(C=1)\n",
    "cls.fit(X_train, y_train)\n",
    "accuracy_score(y_test, cls.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30725462,  0.3038425 ,  0.30137636])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(cls, X_train, y_train, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим свой скорер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top50_accuracy(y_true, y_pred):\n",
    "    n = y_pred.shape[0]\n",
    "    n_take = int(n/2 + 1)\n",
    "    \n",
    "    entropy = np.sum(-y_pred*np.log(y_pred), axis=1)\n",
    "\n",
    "    sort_index = np.argsort(-entropy)\n",
    "    \n",
    "    return accuracy_score(y_true[sort_index][-n_take:], np.argmax(y_pred[sort_index][-n_take:], axis=1)+1)\n",
    "\n",
    "top50_scorer = make_scorer(top50_accuracy, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.36667457,  0.35949727,  0.37390273])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(cls, X_train, y_train, scoring=top50_scorer, n_jobs=1) # Со своим скорером нельзя работать в параллель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно попробовать поискать оптимальное значение C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] C=1e-05 .........................................................\n",
      "[CV] .......................... C=1e-05, score=0.255823, total=   0.9s\n",
      "[CV] C=1e-05 .........................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .......................... C=1e-05, score=0.267088, total=   0.8s\n",
      "[CV] C=1e-05 .........................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .......................... C=1e-05, score=0.260371, total=   0.8s\n",
      "[CV] C=1e-05 .........................................................\n",
      "[CV] .......................... C=1e-05, score=0.259786, total=   0.8s\n",
      "[CV] C=1e-05 .........................................................\n",
      "[CV] .......................... C=1e-05, score=0.257516, total=   0.9s\n",
      "[CV] C=0.0001 ........................................................\n",
      "[CV] ......................... C=0.0001, score=0.257797, total=   1.1s\n",
      "[CV] C=0.0001 ........................................................\n",
      "[CV] ......................... C=0.0001, score=0.267483, total=   1.1s\n",
      "[CV] C=0.0001 ........................................................\n",
      "[CV] ......................... C=0.0001, score=0.261162, total=   1.2s\n",
      "[CV] C=0.0001 ........................................................\n",
      "[CV] ......................... C=0.0001, score=0.262554, total=   1.2s\n",
      "[CV] C=0.0001 ........................................................\n",
      "[CV] ......................... C=0.0001, score=0.259098, total=   1.1s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] .......................... C=0.001, score=0.273983, total=   1.7s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] .......................... C=0.001, score=0.272224, total=   2.0s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] .......................... C=0.001, score=0.272224, total=   1.6s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] .......................... C=0.001, score=0.270463, total=   1.6s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] .......................... C=0.001, score=0.273339, total=   1.7s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] ........................... C=0.01, score=0.307540, total=   2.1s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] ........................... C=0.01, score=0.291584, total=   2.2s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] ........................... C=0.01, score=0.295140, total=   2.1s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] ........................... C=0.01, score=0.288652, total=   2.1s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] ........................... C=0.01, score=0.316060, total=   2.1s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................ C=0.1, score=0.345440, total=   3.1s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................ C=0.1, score=0.330699, total=   3.2s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................ C=0.1, score=0.336626, total=   3.2s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................ C=0.1, score=0.334520, total=   3.2s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................ C=0.1, score=0.365902, total=   3.1s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] .............................. C=1, score=0.368338, total=   6.1s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] .............................. C=1, score=0.362702, total=   6.4s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] .............................. C=1, score=0.360332, total=   6.0s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] .............................. C=1, score=0.364571, total=   6.3s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] .............................. C=1, score=0.390032, total=   6.4s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ............................. C=10, score=0.330043, total=  14.8s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ............................. C=10, score=0.327934, total=  15.8s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ............................. C=10, score=0.320822, total=  14.6s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ............................. C=10, score=0.335310, total=  16.6s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ............................. C=10, score=0.347706, total=  17.2s\n",
      "[CV] C=100 ...........................................................\n",
      "[CV] ............................ C=100, score=0.294907, total=  33.8s\n",
      "[CV] C=100 ...........................................................\n",
      "[CV] ............................ C=100, score=0.290004, total=  34.8s\n",
      "[CV] C=100 ...........................................................\n",
      "[CV] ............................ C=100, score=0.292375, total=  32.7s\n",
      "[CV] C=100 ...........................................................\n",
      "[CV] ............................ C=100, score=0.285884, total=  31.8s\n",
      "[CV] C=100 ...........................................................\n",
      "[CV] ............................ C=100, score=0.302215, total=  31.6s\n",
      "[CV] C=1000 ..........................................................\n",
      "[CV] ........................... C=1000, score=0.283064, total= 1.2min\n",
      "[CV] C=1000 ..........................................................\n",
      "[CV] ........................... C=1000, score=0.271829, total= 1.3min\n",
      "[CV] C=1000 ..........................................................\n",
      "[CV] ........................... C=1000, score=0.286053, total= 1.2min\n",
      "[CV] C=1000 ..........................................................\n",
      "[CV] ........................... C=1000, score=0.272044, total= 1.0min\n",
      "[CV] C=1000 ..........................................................\n",
      "[CV] ........................... C=1000, score=0.283623, total= 1.2min\n",
      "[CV] C=10000 .........................................................\n",
      "[CV] .......................... C=10000, score=0.273194, total= 1.6min\n",
      "[CV] C=10000 .........................................................\n",
      "[CV] .......................... C=10000, score=0.269064, total= 1.5min\n",
      "[CV] C=10000 .........................................................\n",
      "[CV] .......................... C=10000, score=0.275780, total= 1.4min\n",
      "[CV] C=10000 .........................................................\n",
      "[CV] .......................... C=10000, score=0.269672, total= 1.4min\n",
      "[CV] C=10000 .........................................................\n",
      "[CV] .......................... C=10000, score=0.276503, total= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed: 18.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(top50_accuracy, needs_proba=True), verbose=3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = [10**x for x in range(-5, 5)]\n",
    "gs = GridSearchCV(\n",
    "        LogisticRegression(),\n",
    "        param_grid = {\n",
    "            #'penalty': ['l1', 'l2'],\n",
    "            'C': c\n",
    "        },\n",
    "        scoring=top50_scorer,\n",
    "        verbose=3,\n",
    "        cv=5,\n",
    "        n_jobs=1\n",
    "    )\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36919120747407846"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дефолтное значение лучше всех, его и будем использовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Поищем оптимальные значения min_df и max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_df = 0.5, min_df = 3, mean score = 0.3666876855020253+-0.010453070147189098\n",
      "max_df = 0.5, min_df = 4, mean score = 0.3675566259057925+-0.011917783280156361\n",
      "max_df = 0.5, min_df = 5, mean score = 0.36724063915313493+-0.010662786763540376\n",
      "max_df = 0.5, min_df = 6, mean score = 0.365739131095997+-0.010039935625983595\n",
      "max_df = 0.5, min_df = 7, mean score = 0.3662135018875555+-0.009557253986205168\n",
      "max_df = 0.5, min_df = 8, mean score = 0.3670840987168123+-0.005633742071416733\n",
      "max_df = 0.5, min_df = 9, mean score = 0.3660566491433239+-0.006530857615576467\n",
      "max_df = 0.55, min_df = 3, mean score = 0.3666876855020253+-0.010453070147189098\n",
      "max_df = 0.55, min_df = 4, mean score = 0.3675566259057925+-0.011917783280156361\n",
      "max_df = 0.55, min_df = 5, mean score = 0.36724063915313493+-0.010662786763540376\n",
      "max_df = 0.55, min_df = 6, mean score = 0.365739131095997+-0.010039935625983595\n",
      "max_df = 0.55, min_df = 7, mean score = 0.3662135018875555+-0.009557253986205168\n",
      "max_df = 0.55, min_df = 8, mean score = 0.3670840987168123+-0.005633742071416733\n",
      "max_df = 0.55, min_df = 9, mean score = 0.3660566491433239+-0.006530857615576467\n",
      "max_df = 0.6000000000000001, min_df = 3, mean score = 0.3666876855020253+-0.010453070147189098\n",
      "max_df = 0.6000000000000001, min_df = 4, mean score = 0.3675566259057925+-0.011917783280156361\n",
      "max_df = 0.6000000000000001, min_df = 5, mean score = 0.36724063915313493+-0.010662786763540376\n",
      "max_df = 0.6000000000000001, min_df = 6, mean score = 0.365739131095997+-0.010039935625983595\n",
      "max_df = 0.6000000000000001, min_df = 7, mean score = 0.3662135018875555+-0.009557253986205168\n",
      "max_df = 0.6000000000000001, min_df = 8, mean score = 0.3670840987168123+-0.005633742071416733\n",
      "max_df = 0.6000000000000001, min_df = 9, mean score = 0.3660566491433239+-0.006530857615576467\n",
      "max_df = 0.6500000000000001, min_df = 3, mean score = 0.3673193154164482+-0.013216492538187227\n",
      "max_df = 0.6500000000000001, min_df = 4, mean score = 0.3681894435244024+-0.011757906028505214\n",
      "max_df = 0.6500000000000001, min_df = 5, mean score = 0.36747748062495117+-0.012200692968602692\n",
      "max_df = 0.6500000000000001, min_df = 6, mean score = 0.3666869978017727+-0.011876142567846928\n",
      "max_df = 0.6500000000000001, min_df = 7, mean score = 0.3675568758706803+-0.01071305090630967\n",
      "max_df = 0.6500000000000001, min_df = 8, mean score = 0.36834729627661994+-0.010938658290763962\n",
      "max_df = 0.6500000000000001, min_df = 9, mean score = 0.3671618691163193+-0.010506892546998972\n",
      "max_df = 0.7000000000000002, min_df = 3, mean score = 0.3673193154164482+-0.013216492538187227\n",
      "max_df = 0.7000000000000002, min_df = 4, mean score = 0.3681894435244024+-0.011757906028505214\n",
      "max_df = 0.7000000000000002, min_df = 5, mean score = 0.36747748062495117+-0.012200692968602692\n",
      "max_df = 0.7000000000000002, min_df = 6, mean score = 0.3666869978017727+-0.011876142567846928\n",
      "max_df = 0.7000000000000002, min_df = 7, mean score = 0.3675568758706803+-0.01071305090630967\n",
      "max_df = 0.7000000000000002, min_df = 8, mean score = 0.36834729627661994+-0.010938658290763962\n",
      "max_df = 0.7000000000000002, min_df = 9, mean score = 0.3671618691163193+-0.010506892546998972\n",
      "max_df = 0.7500000000000002, min_df = 3, mean score = 0.3683473587680176+-0.011888231535307065\n",
      "max_df = 0.7500000000000002, min_df = 4, mean score = 0.36953366095397755+-0.010937544067292852\n",
      "max_df = 0.7500000000000002, min_df = 5, mean score = 0.3695326610199161+-0.012814604682277497\n",
      "max_df = 0.7500000000000002, min_df = 6, mean score = 0.36866287687294585+-0.012757570275676979\n",
      "max_df = 0.7500000000000002, min_df = 7, mean score = 0.36842650375222347+-0.01049974809203959\n",
      "max_df = 0.7500000000000002, min_df = 8, mean score = 0.36992857452705286+-0.009680533920929806\n",
      "max_df = 0.7500000000000002, min_df = 9, mean score = 0.36811029816940216+-0.011400116578844221\n",
      "max_df = 0.8000000000000003, min_df = 3, mean score = 0.3683473587680176+-0.011888231535307065\n",
      "max_df = 0.8000000000000003, min_df = 4, mean score = 0.36953366095397755+-0.010937544067292852\n",
      "max_df = 0.8000000000000003, min_df = 5, mean score = 0.3695326610199161+-0.012814604682277497\n",
      "max_df = 0.8000000000000003, min_df = 6, mean score = 0.36866287687294585+-0.012757570275676979\n",
      "max_df = 0.8000000000000003, min_df = 7, mean score = 0.36842650375222347+-0.01049974809203959\n",
      "max_df = 0.8000000000000003, min_df = 8, mean score = 0.36992857452705286+-0.009680533920929806\n",
      "max_df = 0.8000000000000003, min_df = 9, mean score = 0.36811029816940216+-0.011400116578844221\n"
     ]
    }
   ],
   "source": [
    "y = train_df['class'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df['url_words'], y, test_size=0.3)\n",
    "trace = []  \n",
    "    \n",
    "for max_df in np.arange(0.5, 0.8, 0.05):\n",
    "    for min_df in range(3, 10):\n",
    "        tfidf = TfidfVectorizer(min_df=min_df, max_df=max_df)\n",
    "        tfidf.fit(df.url_words)\n",
    "        domain_words_features = tfidf.transform(X_train)\n",
    "        cls = LogisticRegression(C=1)\n",
    "        scores = cross_val_score(cls, domain_words_features, y_train, scoring=top50_scorer, n_jobs=1, cv=5)\n",
    "        trace.append({'max_df': max_df, 'min_df': min_df, 'scores': scores})\n",
    "        print('max_df = {}, min_df = {}, mean score = {}+-{}'.format(max_df, min_df, scores.mean(), 2*scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_score</th>\n",
       "      <th>max_df</th>\n",
       "      <th>min_df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.369929</td>\n",
       "      <td>0.80</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.369929</td>\n",
       "      <td>0.75</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.369534</td>\n",
       "      <td>0.75</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.369534</td>\n",
       "      <td>0.80</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.369533</td>\n",
       "      <td>0.80</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    avg_score  max_df  min_df\n",
       "47   0.369929    0.80       8\n",
       "40   0.369929    0.75       8\n",
       "36   0.369534    0.75       4\n",
       "43   0.369534    0.80       4\n",
       "44   0.369533    0.80       5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([{'max_df': d['max_df'], 'min_df':d['min_df'], 'avg_score': d['scores'].mean()} for d in trace]).sort_values('avg_score', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, возьмем лучшие параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=8, max_df=0.8)\n",
    "tfidf.fit(df.url_words)\n",
    "domain_words_features = tfidf.transform(train_df.url_words)\n",
    "y = train_df['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно немного поднять качество через BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=30, n_jobs=-1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "be = BaggingClassifier(base_estimator=LogisticRegression(), n_jobs=-1, n_estimators=30)\n",
    "\n",
    "be.fit(domain_words_features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=8, max_df=0.8)\n",
    "tfidf.fit(df.url_words)\n",
    "domain_words_features = tfidf.transform(train_df.url_words)\n",
    "y = train_df['class'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(domain_words_features, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[CV] n_estimators=10 .................................................\n",
      "[CV] .................. n_estimators=10, score=0.344787, total=  58.0s\n",
      "[CV] n_estimators=10 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   59.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. n_estimators=10, score=0.363098, total=  58.3s\n",
      "[CV] n_estimators=10 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. n_estimators=10, score=0.356126, total=  58.0s\n",
      "[CV] n_estimators=10 .................................................\n",
      "[CV] .................. n_estimators=10, score=0.363241, total=  57.2s\n",
      "[CV] n_estimators=10 .................................................\n",
      "[CV] .................. n_estimators=10, score=0.355872, total=  55.9s\n",
      "[CV] n_estimators=20 .................................................\n",
      "[CV] .................. n_estimators=20, score=0.349526, total= 1.8min\n",
      "[CV] n_estimators=20 .................................................\n",
      "[CV] .................. n_estimators=20, score=0.366653, total= 1.9min\n",
      "[CV] n_estimators=20 .................................................\n",
      "[CV] .................. n_estimators=20, score=0.358103, total= 1.9min\n",
      "[CV] n_estimators=20 .................................................\n",
      "[CV] .................. n_estimators=20, score=0.364427, total= 1.9min\n",
      "[CV] n_estimators=20 .................................................\n",
      "[CV] .................. n_estimators=20, score=0.354686, total= 1.9min\n",
      "[CV] n_estimators=30 .................................................\n",
      "[CV] .................. n_estimators=30, score=0.358610, total= 2.8min\n",
      "[CV] n_estimators=30 .................................................\n",
      "[CV] .................. n_estimators=30, score=0.368234, total= 3.0min\n",
      "[CV] n_estimators=30 .................................................\n",
      "[CV] .................. n_estimators=30, score=0.361265, total= 3.0min\n",
      "[CV] n_estimators=30 .................................................\n",
      "[CV] .................. n_estimators=30, score=0.369565, total= 3.0min\n",
      "[CV] n_estimators=30 .................................................\n",
      "[CV] .................. n_estimators=30, score=0.350336, total= 3.0min\n",
      "[CV] n_estimators=40 .................................................\n",
      "[CV] .................. n_estimators=40, score=0.356240, total= 3.9min\n",
      "[CV] n_estimators=40 .................................................\n",
      "[CV] .................. n_estimators=40, score=0.365468, total= 3.7min\n",
      "[CV] n_estimators=40 .................................................\n",
      "[CV] .................. n_estimators=40, score=0.359684, total= 3.8min\n",
      "[CV] n_estimators=40 .................................................\n",
      "[CV] .................. n_estimators=40, score=0.358103, total= 3.8min\n",
      "[CV] n_estimators=40 .................................................\n",
      "[CV] .................. n_estimators=40, score=0.360221, total= 3.8min\n",
      "[CV] n_estimators=50 .................................................\n",
      "[CV] .................. n_estimators=50, score=0.352291, total= 4.6min\n",
      "[CV] n_estimators=50 .................................................\n",
      "[CV] .................. n_estimators=50, score=0.362307, total= 4.7min\n",
      "[CV] n_estimators=50 .................................................\n",
      "[CV] .................. n_estimators=50, score=0.361660, total= 4.8min\n",
      "[CV] n_estimators=50 .................................................\n",
      "[CV] .................. n_estimators=50, score=0.369960, total= 4.7min\n",
      "[CV] n_estimators=50 .................................................\n",
      "[CV] .................. n_estimators=50, score=0.353499, total= 4.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed: 72.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=BaggingClassifier(base_estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start...n_estimators=10, n_jobs=1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [10, 20, 30, 40, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(top50_accuracy, needs_proba=True), verbose=3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsbe = GridSearchCV(\n",
    "        BaggingClassifier(base_estimator=LogisticRegression()),\n",
    "        param_grid = {\n",
    "            #'penalty': ['l1', 'l2'],\n",
    "            'n_estimators': [10, 20, 30, 40, 50]\n",
    "        },\n",
    "        scoring=top50_scorer,\n",
    "        verbose=3,\n",
    "        cv=5,\n",
    "        n_jobs=1\n",
    "    )\n",
    "gsbe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 30}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsbe.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим с лучшими параметрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "be = BaggingClassifier(base_estimator=LogisticRegression(), n_jobs=-1, n_estimators=30)\n",
    "\n",
    "be.fit(domain_words_features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = tfidf.transform(test_df.url_words)\n",
    "y_hat = be.predict(test_features)\n",
    "probs = be.predict_proba(test_features)\n",
    "entropy = np.sum(probs*np.log(probs), axis=1)\n",
    "pred = pd.DataFrame({'uid': test_df.uid, \n",
    "                     'y_hat': y_hat, \n",
    "                     'entropy': entropy})\n",
    "pred['class'] = le.inverse_transform(pred.y_hat)\n",
    "pred['gender'] = pred['class'].str[0:1]\n",
    "pred['age'] = pred['class'].str[1:]\n",
    "n_pred = pred.shape[0]\n",
    "sure_pred = pred.sort_values('entropy', ascending=False).reset_index()\n",
    "sure_pred.loc[(n_pred/2+1):, 'gender'] = '-'\n",
    "sure_pred.loc[(n_pred/2+1):, 'age'] = '-'\n",
    "sure_pred[['uid', 'gender', 'age']].sort_values('uid').to_csv('project01_gender-age.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.366253498601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Содержимое сайтов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим информацию из содержимого сайтов. В файле url10_words.txt для каждого url хранится top-10 самых частых слов из контента страницы. Thanks to alexander.slastnykh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/url10_words.txt', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    content_df = pd.read_csv(f, sep='\\t', names=['url', 'top10'], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(532605, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>url</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://p-tools.ru/index.php</th>\n",
       "      <td>труба оборудование фитинг инструмент комплекто...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://films.imhonet.ru/element/9776440</th>\n",
       "      <td>nginx found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://makler.md/ru/an/user/index/id/871265</th>\n",
       "      <td>сайт войти регион регистрация приднестровье мо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://smintorg.ru/products/category/833184</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://joyreactor.cc/tag/nsfw/20</th>\n",
       "      <td>nsfw ссылка 2014 комментарий развернуть картин...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         top10\n",
       "url                                                                                           \n",
       "http://p-tools.ru/index.php                  труба оборудование фитинг инструмент комплекто...\n",
       "http://films.imhonet.ru/element/9776440                                           nginx found \n",
       "http://makler.md/ru/an/user/index/id/871265  сайт войти регион регистрация приднестровье мо...\n",
       "http://smintorg.ru/products/category/833184                                                NaN\n",
       "http://joyreactor.cc/tag/nsfw/20             nsfw ссылка 2014 комментарий развернуть картин..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_url_content_words(url):\n",
    "    try:\n",
    "        url = url.lower()\n",
    "        words = content_df.loc[url, 'top10'].strip()\n",
    "        words = re.split(r'\\W|_|-', words)\n",
    "    except:\n",
    "        return []\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nginx', 'found']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_url_content_words('http://films.imhonet.ru/element/9776440')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlsplit\n",
    "\n",
    "def url2domain2(url):\n",
    "    url = url.lower()\n",
    "    url = re.sub('(http(s)*://)+', 'http://', url)\n",
    "    parsed_url = urlparse(unquote(url.strip()))\n",
    "    if parsed_url.scheme not in ['http','https']: \n",
    "        return None\n",
    "    netloc = re.search(\"(?:www\\.)?(.*)\", parsed_url.netloc).group(1)\n",
    "    if netloc is not None: \n",
    "        netloc = netloc.strip()\n",
    "        \n",
    "        # Обрабатываем кириллические домены\n",
    "        if '.xn--' in netloc or netloc.startswith('xn--'):\n",
    "            netloc = netloc.encode('idna').decode('idna')\n",
    "            \n",
    "        # Обрабатываем referrer\n",
    "        r = re.match(r'&referrer=(.+)', netloc)\n",
    "        if r:\n",
    "            netloc = r[1]\n",
    "            \n",
    "        return netloc.strip()\n",
    "    return None\n",
    "\n",
    "def get_domain_words(netloc):\n",
    "    if netloc is not None: \n",
    "        netloc = netloc.strip()\n",
    "        \n",
    "        # Обрабатываем кириллические домены\n",
    "        if '.xn--' in netloc or netloc.startswith('xn--'):\n",
    "            netloc = netloc.encode('idna').decode('idna')\n",
    "            \n",
    "        # Обрабатываем referrer\n",
    "        r = re.match(r'&referrer=(.+)', netloc)\n",
    "        if r:\n",
    "            netloc = r[1]\n",
    "            \n",
    "        return re.split(r'\\W|_|-', netloc.strip())\n",
    "    return []\n",
    "    \n",
    "\n",
    "def get_url_words(url):\n",
    "    url = url.lower()\n",
    "    url = re.sub('(http(s)*://)+', 'http://', url)\n",
    "    \n",
    "    if 'yandex.ru/clck/jsredir' in url:\n",
    "        return ''\n",
    "    parsed_url = urlparse(unquote(url.strip()))\n",
    "    if parsed_url.scheme not in ['http','https']: \n",
    "        return None\n",
    "    netloc = re.search(\"(?:www\\.)?(.*)\", parsed_url.netloc).group(1)\n",
    "    \n",
    "    # Разбиваем на слова домены\n",
    "    words = get_domain_words(netloc)\n",
    "    \n",
    "    # Разбиваем на слова параметры запроса\n",
    "    query_words = sum([q.split('=') for q in parsed_url.query.split('&')], [])\n",
    "    query_words = sum([re.split('\\W|_|-', q) for q in query_words], [])\n",
    "    \n",
    "    # Разбиваем путь на слова\n",
    "    path_words = re.split(r'\\W|_|-', parsed_url.path)\n",
    "    \n",
    "    all_words = [w for w in (words + query_words + path_words) if len(w) >= 3 and not re.match(r'^[0-9]+$', w)]\n",
    "    \n",
    "    content_words = get_url_content_words(url)\n",
    "    \n",
    "    all_words = list(set(all_words + content_words))\n",
    "    #all_words = list(set(content_words))\n",
    "    \n",
    "    return ' '.join(all_words)\n",
    "\n",
    "def extract_words(visits):\n",
    "    \"\"\"\n",
    "    Получить множество всех слов из лога пользователя\n",
    "    \"\"\"\n",
    "    return {get_url_words(visit['url']) for visit in visits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  судьба эфир 2017 трейлер войти человек смотреть октябрь серия  сайт добавить 2017 сообщество отзыв квартира друг свой вопрос  здравствовать дата писали morpher сервис словосочетание склонение оценка веб http вакансия работодатель найти возможность поиск резюме rabota город работа wishlist cart your have product products home price that life  написать телефон разработчик alert сайт поиск знакомство правило фото android телефон интерисуета ответ спросить прошивка категория аноним root вопрос'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(extract_words(df.iloc[1].user_json['visits']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['url_words'] = df.user_json.apply(lambda j: ' '.join(extract_words(j['visits'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = df[~((df.gender == '-') & (df.age == '-'))]\n",
    "test_df = df[(df.gender == '-') & (df.age == '-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=8, max_df=0.8)\n",
    "tfidf.fit(df.url_words)\n",
    "domain_words_features = tfidf.transform(train_df.url_words)\n",
    "y = train_df['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=30, n_jobs=-1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "be = BaggingClassifier(base_estimator=LogisticRegression(), n_jobs=-1, n_estimators=30)\n",
    "\n",
    "be.fit(domain_words_features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = tfidf.transform(test_df.url_words)\n",
    "y_hat = be.predict(test_features)\n",
    "probs = be.predict_proba(test_features)\n",
    "entropy = np.sum(probs*np.log(probs), axis=1)\n",
    "pred = pd.DataFrame({'uid': test_df.uid, \n",
    "                     'y_hat': y_hat, \n",
    "                     'entropy': entropy})\n",
    "pred['class'] = le.inverse_transform(pred.y_hat)\n",
    "pred['gender'] = pred['class'].str[0:1]\n",
    "pred['age'] = pred['class'].str[1:]\n",
    "n_pred = pred.shape[0]\n",
    "sure_pred = pred.sort_values('entropy', ascending=False).reset_index()\n",
    "sure_pred.loc[(n_pred/2+1):, 'gender'] = '-'\n",
    "sure_pred.loc[(n_pred/2+1):, 'age'] = '-'\n",
    "sure_pred[['uid', 'gender', 'age']].sort_values('uid').to_csv('project01_gender-age.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_df = 0.5, min_df = 3, mean score = 0.3055648018243284+-0.0139753399832169\n",
      "max_df = 0.5, min_df = 4, mean score = 0.3069874775749102+-0.015211735482524984\n",
      "max_df = 0.5, min_df = 5, mean score = 0.30603839212940465+-0.014015001604614266\n",
      "max_df = 0.5, min_df = 6, mean score = 0.30493229720520143+-0.018482477188528974\n",
      "max_df = 0.5, min_df = 7, mean score = 0.30516913922003946+-0.015492818708813512\n",
      "max_df = 0.5, min_df = 8, mean score = 0.3058812264101851+-0.017950778936853987\n",
      "max_df = 0.5, min_df = 9, mean score = 0.3065928451505831+-0.01818613030549902\n",
      "max_df = 0.55, min_df = 3, mean score = 0.3055648018243284+-0.0139753399832169\n",
      "max_df = 0.55, min_df = 4, mean score = 0.3069874775749102+-0.015211735482524984\n",
      "max_df = 0.55, min_df = 5, mean score = 0.30603839212940465+-0.014015001604614266\n",
      "max_df = 0.55, min_df = 6, mean score = 0.30493229720520143+-0.018482477188528974\n",
      "max_df = 0.55, min_df = 7, mean score = 0.30516913922003946+-0.015492818708813512\n",
      "max_df = 0.55, min_df = 8, mean score = 0.3058812264101851+-0.017950778936853987\n",
      "max_df = 0.55, min_df = 9, mean score = 0.3065928451505831+-0.01818613030549902\n",
      "max_df = 0.6000000000000001, min_df = 3, mean score = 0.3055648018243284+-0.0139753399832169\n",
      "max_df = 0.6000000000000001, min_df = 4, mean score = 0.3069874775749102+-0.015211735482524984\n",
      "max_df = 0.6000000000000001, min_df = 5, mean score = 0.30603839212940465+-0.014015001604614266\n",
      "max_df = 0.6000000000000001, min_df = 6, mean score = 0.30493229720520143+-0.018482477188528974\n",
      "max_df = 0.6000000000000001, min_df = 7, mean score = 0.30516913922003946+-0.015492818708813512\n",
      "max_df = 0.6000000000000001, min_df = 8, mean score = 0.3058812264101851+-0.017950778936853987\n",
      "max_df = 0.6000000000000001, min_df = 9, mean score = 0.3065928451505831+-0.01818613030549902\n",
      "max_df = 0.6500000000000001, min_df = 3, mean score = 0.3055648018243284+-0.0139753399832169\n",
      "max_df = 0.6500000000000001, min_df = 4, mean score = 0.3069874775749102+-0.015211735482524984\n",
      "max_df = 0.6500000000000001, min_df = 5, mean score = 0.30603839212940465+-0.014015001604614266\n",
      "max_df = 0.6500000000000001, min_df = 6, mean score = 0.30493229720520143+-0.018482477188528974\n",
      "max_df = 0.6500000000000001, min_df = 7, mean score = 0.30516913922003946+-0.015492818708813512\n",
      "max_df = 0.6500000000000001, min_df = 8, mean score = 0.3058812264101851+-0.017950778936853987\n",
      "max_df = 0.6500000000000001, min_df = 9, mean score = 0.3065928451505831+-0.01818613030549902\n",
      "max_df = 0.7000000000000002, min_df = 3, mean score = 0.3055648018243284+-0.0139753399832169\n",
      "max_df = 0.7000000000000002, min_df = 4, mean score = 0.3069874775749102+-0.015211735482524984\n",
      "max_df = 0.7000000000000002, min_df = 5, mean score = 0.30603839212940465+-0.014015001604614266\n",
      "max_df = 0.7000000000000002, min_df = 6, mean score = 0.30493229720520143+-0.018482477188528974\n",
      "max_df = 0.7000000000000002, min_df = 7, mean score = 0.30516913922003946+-0.015492818708813512\n",
      "max_df = 0.7000000000000002, min_df = 8, mean score = 0.3058812264101851+-0.017950778936853987\n",
      "max_df = 0.7000000000000002, min_df = 9, mean score = 0.3065928451505831+-0.01818613030549902\n",
      "max_df = 0.7500000000000002, min_df = 3, mean score = 0.3055648018243284+-0.0139753399832169\n",
      "max_df = 0.7500000000000002, min_df = 4, mean score = 0.3069874775749102+-0.015211735482524984\n",
      "max_df = 0.7500000000000002, min_df = 5, mean score = 0.30603839212940465+-0.014015001604614266\n",
      "max_df = 0.7500000000000002, min_df = 6, mean score = 0.30493229720520143+-0.018482477188528974\n",
      "max_df = 0.7500000000000002, min_df = 7, mean score = 0.30516913922003946+-0.015492818708813512\n",
      "max_df = 0.7500000000000002, min_df = 8, mean score = 0.3058812264101851+-0.017950778936853987\n",
      "max_df = 0.7500000000000002, min_df = 9, mean score = 0.3065928451505831+-0.01818613030549902\n",
      "max_df = 0.8000000000000003, min_df = 3, mean score = 0.3055648018243284+-0.0139753399832169\n",
      "max_df = 0.8000000000000003, min_df = 4, mean score = 0.3069874775749102+-0.015211735482524984\n",
      "max_df = 0.8000000000000003, min_df = 5, mean score = 0.30603839212940465+-0.014015001604614266\n",
      "max_df = 0.8000000000000003, min_df = 6, mean score = 0.30493229720520143+-0.018482477188528974\n",
      "max_df = 0.8000000000000003, min_df = 7, mean score = 0.30516913922003946+-0.015492818708813512\n",
      "max_df = 0.8000000000000003, min_df = 8, mean score = 0.3058812264101851+-0.017950778936853987\n",
      "max_df = 0.8000000000000003, min_df = 9, mean score = 0.3065928451505831+-0.01818613030549902\n"
     ]
    }
   ],
   "source": [
    "y = train_df['class'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df['url_words'], y, test_size=0.3)\n",
    "trace = []  \n",
    "    \n",
    "for max_df in np.arange(0.5, 0.8, 0.05):\n",
    "    for min_df in range(3, 10):\n",
    "        tfidf = TfidfVectorizer(min_df=min_df, max_df=max_df)\n",
    "        tfidf.fit(df.url_words)\n",
    "        domain_words_features = tfidf.transform(X_train)\n",
    "        cls = LogisticRegression(C=1)\n",
    "        scores = cross_val_score(cls, domain_words_features, y_train, scoring=top50_scorer, n_jobs=1, cv=5)\n",
    "        trace.append({'max_df': max_df, 'min_df': min_df, 'scores': scores})\n",
    "        print('max_df = {}, min_df = {}, mean score = {}+-{}'.format(max_df, min_df, scores.mean(), 2*scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=8, max_df=10000)\n",
    "tfidf.fit(df.url_words)\n",
    "domain_words_features = tfidf.transform(train_df.url_words)\n",
    "y = train_df['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36138, 71807)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_words_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Категории сайтов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть еще информация о категориях сайтов, в файле dom_cat.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domcat_df = pd.read_csv('data/dom_cat.csv', sep=';', encoding='latin1')\n",
    "domcat_df['domain '] = domcat_df['domain '].str.strip()\n",
    "domcat_df.set_index('domain ', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domain</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sotovik.ru</th>\n",
       "      <td>Computers/Mobile/Cellular Computers/Computers/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news.yandex.ru</th>\n",
       "      <td>Employment/Staff Media/Online_Media Media/News...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zebra-zoya.ru</th>\n",
       "      <td>Business/Office/General Business/Production/Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enter.ru</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>euroavtoprokat.ru</th>\n",
       "      <td>Rest/Travels/Travel_Agencies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            category\n",
       "domain                                                              \n",
       "sotovik.ru         Computers/Mobile/Cellular Computers/Computers/...\n",
       "news.yandex.ru     Employment/Staff Media/Online_Media Media/News...\n",
       "zebra-zoya.ru      Business/Office/General Business/Production/Co...\n",
       "enter.ru                                                         NaN\n",
       "euroavtoprokat.ru                       Rest/Travels/Travel_Agencies"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domcat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_url_category_words(url):\n",
    "    try:\n",
    "        url = url.lower()\n",
    "        domain = url2domain2(url)\n",
    "        words = domcat_df.loc[domain, 'category'].strip().split()\n",
    "    except:\n",
    "        return []\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Automobiles/Purchasing', 'Computers/Internet/Ratings', 'Computers/Computers']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_url_category_words('http://films.imhonet.ru/element/9776440')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlsplit\n",
    "\n",
    "def url2domain2(url):\n",
    "    url = url.lower()\n",
    "    url = re.sub('(http(s)*://)+', 'http://', url)\n",
    "    parsed_url = urlparse(unquote(url.strip()))\n",
    "    if parsed_url.scheme not in ['http','https']: \n",
    "        return None\n",
    "    netloc = re.search(\"(?:www\\.)?(.*)\", parsed_url.netloc).group(1)\n",
    "    if netloc is not None: \n",
    "        netloc = netloc.strip()\n",
    "        \n",
    "        # Обрабатываем кириллические домены\n",
    "        if '.xn--' in netloc or netloc.startswith('xn--'):\n",
    "            netloc = netloc.encode('idna').decode('idna')\n",
    "            \n",
    "        # Обрабатываем referrer\n",
    "        r = re.match(r'&referrer=(.+)', netloc)\n",
    "        if r:\n",
    "            netloc = r[1]\n",
    "            \n",
    "        return netloc.strip()\n",
    "    return None\n",
    "\n",
    "def get_domain_words(netloc):\n",
    "    if netloc is not None: \n",
    "        netloc = netloc.strip()\n",
    "        \n",
    "        # Обрабатываем кириллические домены\n",
    "        if '.xn--' in netloc or netloc.startswith('xn--'):\n",
    "            netloc = netloc.encode('idna').decode('idna')\n",
    "            \n",
    "        # Обрабатываем referrer\n",
    "        r = re.match(r'&referrer=(.+)', netloc)\n",
    "        if r:\n",
    "            netloc = r[1]\n",
    "            \n",
    "        return re.split(r'\\W|_|-', netloc.strip())\n",
    "    return []\n",
    "    \n",
    "\n",
    "def get_url_words(url):\n",
    "    url = url.lower()\n",
    "    url = re.sub('(http(s)*://)+', 'http://', url)\n",
    "    \n",
    "    if 'yandex.ru/clck/jsredir' in url:\n",
    "        return ''\n",
    "    parsed_url = urlparse(unquote(url.strip()))\n",
    "    if parsed_url.scheme not in ['http','https']: \n",
    "        return None\n",
    "    netloc = re.search(\"(?:www\\.)?(.*)\", parsed_url.netloc).group(1)\n",
    "    \n",
    "    # Разбиваем на слова домены\n",
    "    words = get_domain_words(netloc)\n",
    "    \n",
    "    # Разбиваем на слова параметры запроса\n",
    "    query_words = sum([q.split('=') for q in parsed_url.query.split('&')], [])\n",
    "    query_words = sum([re.split('\\W|_|-', q) for q in query_words], [])\n",
    "    \n",
    "    # Разбиваем путь на слова\n",
    "    path_words = re.split(r'\\W|_|-', parsed_url.path)\n",
    "    \n",
    "    all_words = [w for w in (words + query_words + path_words) if len(w) >= 3 and not re.match(r'^[0-9]+$', w)]\n",
    "    \n",
    "    content_words = get_url_content_words(url)\n",
    "    domcat_words = get_url_category_words(url)\n",
    "    \n",
    "    all_words = list(set(all_words + content_words + domcat_words))\n",
    "    #all_words = list(set(content_words))\n",
    "    \n",
    "    return ' '.join(all_words)\n",
    "\n",
    "def extract_words(visits):\n",
    "    \"\"\"\n",
    "    Получить множество всех слов из лога пользователя\n",
    "    \"\"\"\n",
    "    return {get_url_words(visit['url']) for visit in visits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Private_Life/Housing/4760 vodi poverka kiev Business/Construction/Infrastructure html vizar Business/Production/Electronics cchetchikov net filmy kinogo cpa com source eldorado uid 6204424ffed096fb1453b8a657d4f0a3 admitad medium utm com news Society/NGO Society/synt2/Advice posovesti aspx Automobiles/Law newsid'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(extract_words(df.iloc[20].user_json['visits']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['url_words'] = df.user_json.apply(lambda j: ' '.join(extract_words(j['visits'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = df[~((df.gender == '-') & (df.age == '-'))]\n",
    "test_df = df[(df.gender == '-') & (df.age == '-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_df = 0.1, min_df = 2, n_features = 265719, mean score = 0.3742502851846748+-0.021215842674886843\n",
      "max_df = 0.1, min_df = 3, n_features = 172319, mean score = 0.3745667405597174+-0.020015622452643198\n",
      "max_df = 0.1, min_df = 5, n_features = 107676, mean score = 0.3737762889453634+-0.019419898871968388\n",
      "max_df = 0.1, min_df = 10, n_features = 59666, mean score = 0.3737763200551532+-0.017057790091806826\n",
      "max_df = 0.5, min_df = 2, n_features = 265853, mean score = 0.37638395422053045+-0.013243884059564505\n",
      "max_df = 0.5, min_df = 3, n_features = 172453, mean score = 0.3758300637439082+-0.01273345074770537\n",
      "max_df = 0.5, min_df = 5, n_features = 107810, mean score = 0.37590986462694487+-0.011447952534714355\n",
      "max_df = 0.5, min_df = 10, n_features = 59800, mean score = 0.37646216233020435+-0.009048641231812003\n",
      "max_df = 0.7, min_df = 2, n_features = 265855, mean score = 0.3775696940345417+-0.01443633354601965\n",
      "max_df = 0.7, min_df = 3, n_features = 172455, mean score = 0.3772527075456872+-0.009479703081084135\n",
      "max_df = 0.7, min_df = 5, n_features = 107812, mean score = 0.3770953231687364+-0.010532639305916363\n",
      "max_df = 0.7, min_df = 10, n_features = 59802, mean score = 0.37717318749032713+-0.007992565521537405\n",
      "max_df = 0.8, min_df = 2, n_features = 265856, mean score = 0.3776487141599506+-0.012333035278296892\n",
      "max_df = 0.8, min_df = 3, n_features = 172456, mean score = 0.37717428087708216+-0.0119160156050292\n",
      "max_df = 0.8, min_df = 5, n_features = 107813, mean score = 0.3774907674113148+-0.011332906392363108\n",
      "max_df = 0.8, min_df = 10, n_features = 59803, mean score = 0.377806160805764+-0.007599645874047537\n"
     ]
    }
   ],
   "source": [
    "y = train_df['class'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df['url_words'], y, test_size=0.3)\n",
    "trace = []  \n",
    "    \n",
    "for max_df in [0.1, 0.5, 0.7, 0.8]:\n",
    "    for min_df in [2, 3, 5, 10]:\n",
    "        tfidf = TfidfVectorizer(min_df=min_df, max_df=max_df, sublinear_tf=True)\n",
    "        tfidf.fit(df.url_words)\n",
    "        domain_words_features = tfidf.transform(X_train)\n",
    "        n_features = domain_words_features.shape[1]\n",
    "        cls = LogisticRegression(C=1)\n",
    "        scores = cross_val_score(cls, domain_words_features, y_train, scoring=top50_scorer, n_jobs=1, cv=5)\n",
    "        trace.append({'max_df': max_df, 'min_df': min_df, 'scores': scores})\n",
    "        print('max_df = {}, min_df = {}, n_features = {}, mean score = {}+-{}'.format(max_df, min_df, n_features, scores.mean(), 2*scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем с оптимальными параметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "tfidf = TfidfVectorizer(min_df=10, max_df=0.8, sublinear_tf=True)\n",
    "tfidf.fit(df.url_words)\n",
    "domain_words_features = tfidf.transform(train_df.url_words)\n",
    "y = train_df['class'].values\n",
    "\n",
    "be = BaggingClassifier(base_estimator=LogisticRegression(), n_jobs=-1, n_estimators=30)\n",
    "\n",
    "be.fit(domain_words_features, y)\n",
    "\n",
    "test_features = tfidf.transform(test_df.url_words)\n",
    "y_hat = be.predict(test_features)\n",
    "probs = be.predict_proba(test_features)\n",
    "entropy = np.sum(probs*np.log(probs), axis=1)\n",
    "pred = pd.DataFrame({'uid': test_df.uid, \n",
    "                     'y_hat': y_hat, \n",
    "                     'entropy': entropy})\n",
    "pred['class'] = le.inverse_transform(pred.y_hat)\n",
    "pred['gender'] = pred['class'].str[0:1]\n",
    "pred['age'] = pred['class'].str[1:]\n",
    "n_pred = pred.shape[0]\n",
    "sure_pred = pred.sort_values('entropy', ascending=False).reset_index()\n",
    "sure_pred.loc[(n_pred/2):, 'gender'] = '-'\n",
    "sure_pred.loc[(n_pred/2):, 'age'] = '-'\n",
    "sure_pred[['uid', 'gender', 'age']].sort_values('uid').to_csv('project01_gender-age.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получил score 0.3796"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "233px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
